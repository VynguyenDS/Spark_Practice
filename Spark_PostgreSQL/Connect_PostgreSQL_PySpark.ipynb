{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established to:  (Decimal('7499'), 'ALLEN', 'SALESMAN', Decimal('7698'), datetime.date(1981, 2, 20), Decimal('1600.00'), Decimal('300.00'), Decimal('30'))\n"
     ]
    }
   ],
   "source": [
    "# Python connect Postgre pd4\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(host=\"localhost\", port = 5432, database=\"sqla\", user=\"postgres\", password=\"admin123\")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#Executing an MYSQL function using the execute() method\n",
    "cursor.execute(\"select * from emp\")\n",
    "\n",
    "# Fetch a single row using fetchone() method.\n",
    "data = cursor.fetchone()\n",
    "print(\"Connection established to: \",data)\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Connect PostgreSQL with pySpark\")\n",
    "         .config(\"spark.jars\", \"postgresql-42.2.20.jar\")\n",
    "         .getOrCreate())\n",
    "# Read file\n",
    "jdbcDF1 = (spark\n",
    "  .read\n",
    "  .format(\"jdbc\") \n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/sqla\")\n",
    "  .option(\"dbtable\", \"emp\")\n",
    "  .option(\"user\", \"postgres\")\n",
    "  .option(\"password\", \"admin123\").option(\"driver\", \"org.postgresql.Driver\").load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+----+----------+-------+-------+------+\n",
      "|empno|   ename|      job| mgr|  hiredate|    sal|   comm|deptno|\n",
      "+-----+--------+---------+----+----------+-------+-------+------+\n",
      "| 7499|   ALLEN| SALESMAN|7698|1981-02-20|1600.00| 300.00|    30|\n",
      "| 7521|    WARD| SALESMAN|7698|1981-02-22|1250.00| 500.00|    30|\n",
      "| 7654|  MARTIN| SALESMAN|7698|1981-09-28|1250.00|1400.00|    30|\n",
      "| 7698|   BLAKE|  MANAGER|7839|1981-05-01|2850.00|   null|    30|\n",
      "| 7782|   CLARK|  MANAGER|7839|1981-06-09|2450.00|   null|    10|\n",
      "| 7839|    KING|PRESIDENT|null|1981-11-17|5000.00|   null|    10|\n",
      "| 7844|  TURNER| SALESMAN|7698|1981-09-08|1500.00|   0.00|    30|\n",
      "| 7900|   JAMES|    CLERK|7698|1981-12-03| 950.00|   null|    30|\n",
      "| 7934|  MILLER|    CLERK|7782|1982-01-23|1300.00|   null|    10|\n",
      "|    1|Jonathan|   Editor|null|      null|   null|   null|  null|\n",
      "| 7369|   SMITH|    CLERK|7902|1980-12-17| 880.00|   null|    20|\n",
      "| 7566|   JONES|  MANAGER|7839|1981-04-02|3272.50|   null|    20|\n",
      "| 7788|   SCOTT|  ANALYST|7566|1982-12-09|3300.00|   null|    20|\n",
      "| 7876|   ADAMS|    CLERK|7788|1983-01-12|1210.00|   null|    20|\n",
      "| 7902|    FORD|  ANALYST|7566|1981-12-03|3300.00|   null|    20|\n",
      "+-----+--------+---------+----+----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write File \n",
    "\n",
    "# Saving data to a JDBC source using save \n",
    "(jdbcDF\n",
    "  .write \n",
    "  .format(\"jdbc\") \n",
    "  .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "  .option(\"driver\", \"com.mysql.jdbc.Driver\") \n",
    "  .option(\"dbtable\", \"[TABLENAME]\") \n",
    "  .option(\"user\", \"[USERNAME]\")\n",
    "  .option(\"password\", \"[PASSWORD]\")\n",
    "  .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform(): transform(array<T>, function<T, U>): array<U>\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    "  FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter(array<T>, function<T, Boolean>): array<T>\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    "  FROM tC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exists(array<T>, function<T, V, Boolean>): Boolean\n",
    "spark.sql(\n",
    "    \"\"\" select celsius, exists(celsius, t -> t =38) as threshold from tC\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# !ls\n",
    "trip_delay_file_path = 'Data/departuredelays.csv'\n",
    "# airport_file_path = \"Data/flights/airport-codes-na.txt\"\n",
    "departure_delays = (\n",
    "    spark.read.format(\"csv\").options(header = True).load(trip_delay_file_path)\n",
    ")\n",
    "departure_delays = (departure_delays\n",
    "    .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    "    .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    ")\n",
    "departure_delays.createOrReplaceTempView(\"departure_delays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = (\n",
    "    departure_delays.filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    "    date like '01010%' and delay > 0\"\"\"))\n",
    ")\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from foo LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union \n",
    "bar = departure_delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "  FROM bar \n",
    " WHERE origin = 'SEA' \n",
    "   AND destination = 'SFO' \n",
    "   AND date LIKE '01010%' \n",
    "   AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins the options being inner, cross, outer, full, full_outer, left, left_outer, right, \n",
    "# right_outer, left_semi, and left_anti\n",
    "# Windowing\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select origin, destination, TotalDelays, dense_rank()\n",
    "    over(parition by origin order by TotalDelays DESC) as rank\n",
    "    from depature_delays\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
