{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark =(\n",
    "    SparkSession.builder.appName(\"Spark_SQL\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile=\"Data/departuredelays.csv\"\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csvFile)\n",
    ")\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to specify a schema, you can use a DDL-formatted string. For example:\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distance, origin, destination\n",
    "    from us_delay_flights_tbl\n",
    "    where distance > 1000\n",
    "    order by distance desc\n",
    "    \"\"\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select date, delay, origin, destination\n",
    "    from us_delay_flights_tbl\n",
    "    where delay > 120 and origin = 'SFO' and destination = 'ORD'\n",
    "    order by delay desc\n",
    "    \"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|flight_delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  long Delays|\n",
      "|  305|   ABE|        ATL|  long Delays|\n",
      "|  275|   ABE|        ATL|  long Delays|\n",
      "|  257|   ABE|        ATL|  long Delays|\n",
      "|  247|   ABE|        DTW|  long Delays|\n",
      "|  247|   ABE|        ATL|  long Delays|\n",
      "|  219|   ABE|        ORD|  long Delays|\n",
      "|  211|   ABE|        ATL|  long Delays|\n",
      "|  197|   ABE|        DTW|  long Delays|\n",
      "|  192|   ABE|        ORD|  long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select delay, origin, destination, \\\n",
    "    case \\\n",
    "        when delay > 360 then 'Very Long Delays' \\\n",
    "        when delay >= 120 and delay <= 360 then 'long Delays' \\\n",
    "        when delay >= 60 and delay < 120 then 'Short Delays' \\\n",
    "        when delay > 0 and delay < 60 then 'Toleable Delays' \\\n",
    "        when delay = 0 then 'No delays'\n",
    "        else 'Early'\n",
    "    end as flight_delays\n",
    "    from us_delay_flights_tbl\n",
    "    order by origin, delay desc\n",
    "    \"\"\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,desc\n",
    "(\n",
    "    df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(col(\"distance\") > 1000)\n",
    "    .orderBy(desc(\"distance\"))).show(10)\n",
    "(\n",
    "    df.select(\"distance\", \"origin\", \"destination\")\n",
    "    .where(\"distance > 1000\")\n",
    "    .orderBy(\"distance\", ascending = False).show(10)\n",
    ")\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL interface to query data is similar to writing a regular SQL query to a relational database table.\n",
    "# Spark manages all the complexities of creating and managing views and tables, both in memory and on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Tables and Views\n",
    "# Tables hold data. Associated with each table in Spark is its relevant metadata, which is information \n",
    "# about the table and its data: the schema, description, table name, database name, column names, partitions, \n",
    "# physical location where the actual data resides, etc. \n",
    "\n",
    "\n",
    "# Spark allows you to create two types of tables: managed and unmanaged. \n",
    "# For a managed table, Spark manages both the metadata and the data in the file store as Amazon S3 or Azure Blob\n",
    "\n",
    "\n",
    "# For an unmanaged table, Spark only manages the metadata, while you manage the data yourself in an \n",
    "# external data source such as Cassandra.\n",
    "\n",
    "\n",
    "# With a managed table, because Spark manages everything, a SQL command such as \n",
    "# DROP TABLE table_name deletes both the metadata and the data.\n",
    "\n",
    "# With an unmanaged table, the same command will delete only the metadata, not the actual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create database from spark sql\n",
    "spark.sql(\"CREATE DATABASE Spark_SQl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE Spark_SQl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a managed table within the database learn_spark_db\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING\"\n",
    "flights_df = spark.read.csv(csvFile, schema=schema)\n",
    "flights_df.write.saveAsTable(\"Delay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an unmanaged table with API dataframe\n",
    "schema = \"`date` STRING, `delay` INT, `distance` INT,`origin` STRING, `destination` STRING\"\n",
    "flights_df = spark.read.csv(csvFile, schema=schema)\n",
    "# flights_df.write.saveAsTable(\"Vynguyen\")\n",
    "(flights_df\n",
    "  .write\n",
    "  .option(\"path\", \"Data/us_delay_flights_tbl\")\n",
    "  .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: int, delay: int, origin: string, destination: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Views\n",
    "# Spark can create views on top of existing tables\n",
    "# Views can be global (visible across all SparkSessions on a given cluster)\n",
    "# Session-scoped (visible only to a single SparkSession)\n",
    "\n",
    "#  The difference between a view and a table is that views don’t actually hold the data; \n",
    "# tables persist after your Spark application terminates, but views disappear.\n",
    "\n",
    "df_sfo = spark.sql(\n",
    "    \"SELECT date, delay, origin, destination \\\n",
    "    from us_delay_flights_tbl where origin = 'SFO' \\\n",
    "    \"\n",
    ")\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM \\\n",
    "  us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\")\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "# drop views\n",
    "# spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "# spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Temporary views versus global temporary views\n",
    "# The difference between temporary and global temporary views being subtle, \n",
    "# it can be a source of mild confusion among developers new to Spark.\n",
    "\n",
    "# A temporary view is tied to a single SparkSession within a Spark application\n",
    "# a global temporary view is visible across multiple SparkSessions within a Spark application\n",
    "\n",
    "# Viewing the Metadata\n",
    "# Spark manages the metadata associated with each managed or unmanaged table\n",
    "# a high-level abstraction in Spark SQL for storing metadata.\n",
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caching SQL Tables\n",
    "# Reading Tables into DataFrames\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")\n",
    "us_flights_df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Sources for DataFrames and SQL Tables\n",
    "# available file formats, and ways to load and write data, along with specific \n",
    "# options pertaining to these data sources. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataFrameReader\n",
    "# DataFrameReader is the core construct for reading data from a data source into a DataFrame.\n",
    "# It has a defined format and a recommended pattern for usage:\n",
    "\n",
    "# DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()\n",
    "\n",
    "# SparkSession.read \n",
    "# // or \n",
    "# SparkSession.readStream\n",
    "\n",
    "\n",
    "# Parquet is the default and preferred data source for Spark because it’s efficient, \n",
    "# uses columnar storage, and employs a fast compression algorithm. \n",
    "# when we cover the Catalyst optimizer in greater depth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameWriter\n",
    "# DataFrameWriter does the reverse of its counterpart:  it saves or writes data to a specified built-in data source\n",
    "\n",
    "# Format:\n",
    "# DataFrameWriter.format(args)\n",
    "#   .option(args)\n",
    "#   .bucketBy(args)\n",
    "#   .partitionBy(args)\n",
    "#   .save(path)\n",
    "\n",
    "# DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)\n",
    "\n",
    "# DataFrame.write\n",
    "# // or \n",
    "# DataFrame.writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "# Parquet is an open source columnar file format that offers many I/O optimizations \n",
    "# (such as compression, which saves storage space and allows for quick access to data columns).\n",
    "\n",
    "# Because of its efficiency and these optimizations, we recommend that after you have transformed and \n",
    "# cleansed your data, you save your DataFrames in the Parquet format for downstream consumption\n",
    "\n",
    "# Reading Parquet files into a DataFrame\n",
    "\n",
    "# file = \"\"\"/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/\\\n",
    "#   2010-summary.parquet/\"\"\"\n",
    "# df = spark.read.format(\"parquet\").load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+------+-----------+\n",
      "|date|delay|distance|origin|destination|\n",
      "+----+-----+--------+------+-----------+\n",
      "+----+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
